{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 4,
>>>>>>> 14ce294 (update_labtop)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "430 160\n"
=======
      "510 140\n",
      "꼬리밟음\n"
>>>>>>> 14ce294 (update_labtop)
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "\n",
    "import pygame\n",
    "import random\n",
    "import time\n",
    "# 게임 초기화\n",
    "pygame.init()\n",
    "screen_width, screen_height = 640, 480\n",
    "screen = pygame.display.set_mode((screen_width, screen_height))\n",
    "pygame.display.set_caption('Snakegame by SJW')\n",
    "clock = pygame.time.Clock()\n",
    "\n",
    "# 색깔 정의\n",
    "BLACK = (0, 0, 0)\n",
    "GREEN = (0, 255, 0)\n",
    "RED = (255, 0, 0)\n",
    "WHITE = (255, 255, 255)\n",
    "\n",
    "# Snake 클래스 정의\n",
    "class Snake:\n",
    "    def __init__(self):\n",
    "        self.x = 10\n",
    "        self.y = 10\n",
    "        self.direction = \"RIGHT\"\n",
    "        self.body = [(self.x, self.y)]\n",
    "\n",
    "    def move(self):\n",
    "        if self.direction == \"RIGHT\":\n",
    "            self.x += 10\n",
    "        elif self.direction == \"LEFT\":\n",
    "            self.x -= 10\n",
    "        elif self.direction == \"UP\":\n",
    "            self.y -= 10\n",
    "        elif self.direction == \"DOWN\":\n",
    "            self.y += 10\n",
    "\n",
    "        self.body.insert(0, (self.x, self.y))\n",
    "        self.body.pop()\n",
    "\n",
    "    def change_direction(self, new_direction):\n",
    "        if new_direction == \"RIGHT\" and self.direction != \"LEFT\":\n",
    "            self.direction = new_direction\n",
    "        elif new_direction == \"LEFT\" and self.direction != \"RIGHT\":\n",
    "            self.direction = new_direction\n",
    "        elif new_direction == \"UP\" and self.direction != \"DOWN\":\n",
    "            self.direction = new_direction\n",
    "        elif new_direction == \"DOWN\" and self.direction != \"UP\":\n",
    "            self.direction = new_direction\n",
    "\n",
    "    def draw(self):\n",
    "        for segment in self.body:\n",
    "            pygame.draw.rect(screen, GREEN, (segment[0], segment[1], 10, 10))\n",
    "\n",
    "# Apple 클래스 정의\n",
    "class Apple:\n",
    "    def __init__(self):\n",
    "        self.x = random.randrange(0, screen_width,10) \n",
    "        self.y = random.randrange(0, screen_height,10) \n",
    "        \n",
    "\n",
    "    def draw(self):\n",
    "        pygame.draw.rect(screen, RED, (self.x, self.y, 10, 10))\n",
    "\n",
    "#Message 출력하는 함수\n",
    "font_style = pygame.font.SysFont(None, 50)\n",
    "def message(msg,color,x_size,y_szie):\n",
    "    mesg = font_style.render(msg,True,color)\n",
    "    screen.blit(mesg,[x_size,y_szie])\n",
    "\n",
    "\n",
    "# 게임 루프\n",
    "running = True\n",
    "snake = Snake()\n",
    "apple = Apple()\n",
    "score = 0\n",
    "\n",
    "#점수 표시\n",
    "score_text = font_style.render(f'SCORE: {score}',True,WHITE) #True를 주면 안티엘라이싱 적용으로 매끄러워보임 \n",
    "score_rect = score_text.get_rect()\n",
    "score_rect.topleft= (10,10)\n",
    "\n",
    "#print(apple.x,apple.y)\n",
    "while running:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            running = False\n",
    "        elif event.type == pygame.KEYDOWN:\n",
    "            if event.key == pygame.K_RIGHT:\n",
    "                snake.change_direction(\"RIGHT\")\n",
    "            elif event.key == pygame.K_LEFT:\n",
    "                snake.change_direction(\"LEFT\")\n",
    "            elif event.key == pygame.K_UP:\n",
    "                snake.change_direction(\"UP\")\n",
    "            elif event.key == pygame.K_DOWN:\n",
    "                snake.change_direction(\"DOWN\")\n",
    "\n",
    "    snake.move()\n",
    "\n",
    "    # 뱀이 벽에 부딪히는 경우 게임 종료\n",
    "    if snake.x < 0 or snake.x > screen_width or snake.y < 0 or snake.y > screen_height:\n",
    "        running = False\n",
    "\n",
    "    #뱀이 꼬리를 밟으면 게임 종료\n",
    "    for elements in snake.body[1:]:\n",
    "        if snake.x == elements[0] and snake.y == elements[1]:\n",
    "            print('꼬리밟음')\n",
    "            running = False\n",
    "\n",
    "    # 뱀이 사과를 먹으면 새로운 사과 생성\n",
    "    if snake.x == apple.x and snake.y == apple.y:\n",
    "        snake.body.append((apple.x, apple.y))\n",
    "        #print('먹었다')\n",
    "        score += 1\n",
    "        apple = Apple()\n",
    "        score_text = font_style.render(\"Score: \" + str(score), True, WHITE)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    screen.fill(BLACK)\n",
    "    snake.draw()\n",
    "    apple.draw()\n",
    "    screen.blit(score_text, score_rect)\n",
    "    pygame.display.flip()\n",
    "    clock.tick(10)\n",
    "\n",
    "message('You Lose',RED,screen_width/2,screen_height/2)\n",
    "pygame.display.update()\n",
    "time.sleep(2)\n",
    "pygame.quit()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN 적용버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.4.0 (SDL 2.26.4, Python 3.8.16)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1  Score: 0\n",
      "Episode: 2  Score: 0\n",
      "Episode: 3  Score: 0\n",
      "Episode: 4  Score: 0\n",
      "Episode: 5  Score: 0\n",
      "Episode: 6  Score: 0\n",
      "Episode: 7  Score: 0\n",
      "Episode: 8  Score: 0\n",
      "Episode: 9  Score: 0\n",
      "Episode: 10  Score: 0\n",
      "Episode: 11  Score: 0\n",
      "Episode: 12  Score: 0\n",
      "Episode: 13  Score: 0\n",
      "Episode: 14  Score: 0\n",
      "Episode: 15  Score: 0\n",
      "Episode: 16  Score: 0\n",
      "Episode: 17  Score: 0\n",
      "Episode: 18  Score: 0\n",
      "Episode: 19  Score: 0\n",
      "Episode: 20  Score: 0\n",
      "Episode: 21  Score: 0\n",
      "Episode: 22  Score: 0\n",
      "Episode: 23  Score: 0\n",
      "Episode: 24  Score: 0\n",
      "Episode: 25  Score: 0\n",
      "Episode: 26  Score: 0\n",
      "Episode: 27  Score: 0\n",
      "Episode: 28  Score: 0\n",
      "Episode: 29  Score: 0\n",
      "Episode: 30  Score: 0\n",
      "Episode: 31  Score: 0\n",
      "Episode: 32  Score: 0\n",
      "Episode: 33  Score: 0\n",
      "Episode: 34  Score: 0\n",
      "Episode: 35  Score: 0\n",
      "Episode: 36  Score: 0\n",
      "Episode: 37  Score: 0\n",
      "Episode: 38  Score: 0\n",
      "Episode: 39  Score: 0\n",
      "Episode: 40  Score: 0\n",
      "Episode: 41  Score: 0\n",
      "Episode: 42  Score: 0\n",
      "Episode: 43  Score: 0\n",
      "Episode: 44  Score: 0\n",
      "Episode: 45  Score: 0\n",
      "Episode: 46  Score: 0\n",
      "Episode: 47  Score: 0\n",
      "Episode: 48  Score: 0\n",
      "Episode: 49  Score: 0\n",
      "Episode: 50  Score: 0\n",
      "Episode: 51  Score: 0\n",
      "Episode: 52  Score: 0\n",
      "Episode: 53  Score: 0\n",
      "Episode: 54  Score: 0\n",
      "Episode: 55  Score: 0\n",
      "Episode: 56  Score: 0\n",
      "Episode: 57  Score: 0\n",
      "Episode: 58  Score: 0\n",
      "Episode: 59  Score: 0\n",
      "Episode: 60  Score: 0\n",
      "Episode: 61  Score: 0\n",
      "Episode: 62  Score: 0\n",
      "Episode: 63  Score: 0\n",
      "Episode: 64  Score: 0\n",
      "Episode: 65  Score: 0\n",
      "Episode: 66  Score: 0\n",
      "Episode: 67  Score: 0\n",
      "Episode: 68  Score: 0\n",
      "Episode: 69  Score: 0\n",
      "Episode: 70  Score: 0\n",
      "Episode: 71  Score: 0\n",
      "Episode: 72  Score: 0\n",
      "Episode: 73  Score: 0\n",
      "Episode: 74  Score: 0\n",
      "Episode: 75  Score: 0\n",
      "Episode: 76  Score: 0\n",
      "Episode: 77  Score: 0\n",
      "Episode: 78  Score: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 173\u001b[0m\n\u001b[0;32m    171\u001b[0m num_episodes \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[0;32m    172\u001b[0m \u001b[39mfor\u001b[39;00m episode \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_episodes):\n\u001b[1;32m--> 173\u001b[0m     score \u001b[39m=\u001b[39m play_game()\n\u001b[0;32m    174\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpisode: \u001b[39m\u001b[39m{\u001b[39;00mepisode\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m  Score: \u001b[39m\u001b[39m{\u001b[39;00mscore\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    176\u001b[0m pygame\u001b[39m.\u001b[39mquit()\n",
      "Cell \u001b[1;32mIn[12], line 166\u001b[0m, in \u001b[0;36mplay_game\u001b[1;34m()\u001b[0m\n\u001b[0;32m    164\u001b[0m     apple\u001b[39m.\u001b[39mdraw()\n\u001b[0;32m    165\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mflip()\n\u001b[1;32m--> 166\u001b[0m     clock\u001b[39m.\u001b[39;49mtick(\u001b[39m10\u001b[39;49m)\n\u001b[0;32m    168\u001b[0m \u001b[39mreturn\u001b[39;00m score\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Pygame initialization\n",
    "pygame.init()\n",
    "screen_width, screen_height = 640, 480\n",
    "screen = pygame.display.set_mode((screen_width, screen_height))\n",
    "pygame.display.set_caption('Snake Game with DQN')\n",
    "clock = pygame.time.Clock()\n",
    "\n",
    "# Colors\n",
    "BLACK = (0, 0, 0)\n",
    "GREEN = (0, 255, 0)\n",
    "RED = (255, 0, 0)\n",
    "WHITE = (255, 255, 255)\n",
    "\n",
    "# Game environment variables\n",
    "grid_size = 20\n",
    "grid_width = screen_width // grid_size\n",
    "grid_height = screen_height // grid_size\n",
    "\n",
    "# DQN-related variables\n",
    "state_size = grid_width * grid_height\n",
    "action_size = 4  # UP, DOWN, LEFT, RIGHT\n",
    "\n",
    "# DQN model\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 24)\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        self.fc3 = nn.Linear(24, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = DQN()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Snake class\n",
    "class Snake:\n",
    "    def __init__(self):\n",
    "        self.x = random.randint(1, grid_width - 2)\n",
    "        self.y = random.randint(1, grid_height - 2)\n",
    "        self.direction = random.choice(['UP', 'DOWN', 'LEFT', 'RIGHT'])\n",
    "        self.body = [(self.x, self.y)]\n",
    "\n",
    "    def move(self):\n",
    "        if self.direction == \"RIGHT\":\n",
    "            self.x += 1\n",
    "        elif self.direction == \"LEFT\":\n",
    "            self.x -= 1\n",
    "        elif self.direction == \"UP\":\n",
    "            self.y -= 1\n",
    "        elif self.direction == \"DOWN\":\n",
    "            self.y += 1\n",
    "\n",
    "        head = (self.x, self.y)\n",
    "        self.body.insert(0, head)\n",
    "        self.body.pop()\n",
    "\n",
    "    def change_direction(self, new_direction):\n",
    "        if new_direction == 'UP' and self.direction != 'DOWN':\n",
    "            self.direction = new_direction\n",
    "        elif new_direction == 'DOWN' and self.direction != 'UP':\n",
    "            self.direction = new_direction\n",
    "        elif new_direction == 'LEFT' and self.direction != 'RIGHT':\n",
    "            self.direction = new_direction\n",
    "        elif new_direction == 'RIGHT' and self.direction != 'LEFT':\n",
    "            self.direction = new_direction\n",
    "\n",
    "    def draw(self):\n",
    "        for segment in self.body:\n",
    "            pygame.draw.rect(screen, GREEN, (segment[0] * grid_size, segment[1] * grid_size, grid_size, grid_size))\n",
    "\n",
    "# Apple class\n",
    "class Apple:\n",
    "    def __init__(self):\n",
    "        self.x = random.randrange(0, screen_width, grid_size)\n",
    "        self.y = random.randrange(0, screen_height, grid_size)\n",
    "\n",
    "    def draw(self):\n",
    "        pygame.draw.rect(screen, RED, (self.x, self.y, grid_size, grid_size))\n",
    "\n",
    "# Game initialization\n",
    "def initialize_game():\n",
    "    snake = Snake()\n",
    "    apple = Apple()\n",
    "    return snake, apple\n",
    "\n",
    "# Get current state\n",
    "def get_state(snake, apple):\n",
    "    state = np.zeros((grid_width, grid_height))\n",
    "    for segment in snake.body:\n",
    "        x, y = segment[0], segment[1]\n",
    "        if x < grid_width and y < grid_height:\n",
    "            state[x, y] = 1\n",
    "    if apple.x < grid_width and apple.y < grid_height:\n",
    "        state[apple.x, apple.y] = 2\n",
    "    return state.flatten()\n",
    "\n",
    "# Select action based on current state\n",
    "def select_action(state):\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float).unsqueeze(0)\n",
    "    q_values = model(state_tensor)\n",
    "    action = torch.argmax(q_values, dim=1).item()\n",
    "    return action\n",
    "\n",
    "# Game over condition\n",
    "def is_game_over(snake):\n",
    "    if snake.x < 0 or snake.x >= grid_width or snake.y < 0 or snake.y >= grid_height:\n",
    "        return True\n",
    "    if (snake.x, snake.y) in snake.body[1:]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Play the game\n",
    "def play_game():\n",
    "    snake, apple = initialize_game()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        state = get_state(snake, apple)\n",
    "        action = select_action(state)\n",
    "        snake.change_direction(['UP', 'DOWN', 'LEFT', 'RIGHT'][action])\n",
    "        snake.move()\n",
    "        if is_game_over(snake):\n",
    "            done = True\n",
    "            reward = -10\n",
    "        elif snake.x == apple.x and snake.y == apple.y:\n",
    "            snake.body.append((apple.x, apple.y))\n",
    "            apple = Apple()\n",
    "            score += 1\n",
    "            reward = 10\n",
    "        else:\n",
    "            reward = -1\n",
    "        next_state = get_state(snake, apple)\n",
    "\n",
    "        # Update Q-values\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float).unsqueeze(0)\n",
    "        next_state_tensor = torch.tensor(next_state, dtype=torch.float).unsqueeze(0)\n",
    "        q_values = model(state_tensor)\n",
    "        next_q_values = model(next_state_tensor)\n",
    "        max_next_q_value = torch.max(next_q_values).item()\n",
    "        q_target = q_values.clone()\n",
    "        q_target[0][action] = reward + 0.99 * max_next_q_value\n",
    "\n",
    "        # Update the model\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(q_values, q_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        screen.fill(BLACK)\n",
    "        snake.draw()\n",
    "        apple.draw()\n",
    "        pygame.display.flip()\n",
    "        clock.tick(10)\n",
    "\n",
    "    return score\n",
    "\n",
    "# Main loop\n",
    "num_episodes = 1000\n",
    "for episode in range(num_episodes):\n",
    "    score = play_game()\n",
    "    print(f\"Episode: {episode + 1}  Score: {score}\")\n",
    "\n",
    "pygame.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 24 but corresponding boolean dimension is 32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 194\u001b[0m\n\u001b[0;32m    192\u001b[0m num_episodes \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[0;32m    193\u001b[0m \u001b[39mfor\u001b[39;00m episode \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_episodes):\n\u001b[1;32m--> 194\u001b[0m     score \u001b[39m=\u001b[39m play_game()\n\u001b[0;32m    195\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpisode: \u001b[39m\u001b[39m{\u001b[39;00mepisode\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m  Score: \u001b[39m\u001b[39m{\u001b[39;00mscore\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    197\u001b[0m pygame\u001b[39m.\u001b[39mquit()\n",
      "Cell \u001b[1;32mIn[14], line 152\u001b[0m, in \u001b[0;36mplay_game\u001b[1;34m()\u001b[0m\n\u001b[0;32m    150\u001b[0m score \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    151\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m--> 152\u001b[0m     state \u001b[39m=\u001b[39m get_state(snake, apple)\n\u001b[0;32m    153\u001b[0m     action \u001b[39m=\u001b[39m select_action(state)\n\u001b[0;32m    154\u001b[0m     snake\u001b[39m.\u001b[39mchange_direction([\u001b[39m'\u001b[39m\u001b[39mUP\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mDOWN\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLEFT\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mRIGHT\u001b[39m\u001b[39m'\u001b[39m][action])\n",
      "Cell \u001b[1;32mIn[14], line 120\u001b[0m, in \u001b[0;36mget_state\u001b[1;34m(snake, apple)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m apple\u001b[39m.\u001b[39mx \u001b[39m<\u001b[39m grid_width \u001b[39mand\u001b[39;00m apple\u001b[39m.\u001b[39my \u001b[39m<\u001b[39m grid_height:\n\u001b[0;32m    119\u001b[0m     state[apple\u001b[39m.\u001b[39my, apple\u001b[39m.\u001b[39mx] \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m  \u001b[39m# Note the change in indexing here\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m \u001b[39mreturn\u001b[39;00m preprocess_state(state)\n",
      "Cell \u001b[1;32mIn[14], line 125\u001b[0m, in \u001b[0;36mpreprocess_state\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_state\u001b[39m(state):\n\u001b[0;32m    124\u001b[0m     preprocessed_state \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((state_channels, state_height, state_width))\n\u001b[1;32m--> 125\u001b[0m     preprocessed_state[\u001b[39m0\u001b[39;49m, :state\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], :state\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m]][state \u001b[39m==\u001b[39;49m \u001b[39m1\u001b[39;49m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m  \u001b[39m# Snake body\u001b[39;00m\n\u001b[0;32m    126\u001b[0m     preprocessed_state[\u001b[39m1\u001b[39m, :state\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], :state\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]][state \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m  \u001b[39m# Apple\u001b[39;00m\n\u001b[0;32m    127\u001b[0m     preprocessed_state[\u001b[39m2\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m  \u001b[39m# Background (set all elements to 1)\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 24 but corresponding boolean dimension is 32"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Pygame initialization\n",
    "pygame.init()\n",
    "screen_width, screen_height = 640, 480\n",
    "screen = pygame.display.set_mode((screen_width, screen_height))\n",
    "pygame.display.set_caption('Snake Game with DQN (CNN)')\n",
    "clock = pygame.time.Clock()\n",
    "\n",
    "# Colors\n",
    "BLACK = (0, 0, 0)\n",
    "GREEN = (0, 255, 0)\n",
    "RED = (255, 0, 0)\n",
    "WHITE = (255, 255, 255)\n",
    "\n",
    "# Game environment variables\n",
    "grid_size = 20\n",
    "grid_width = screen_width // grid_size\n",
    "grid_height = screen_height // grid_size\n",
    "\n",
    "# DQN-related variables\n",
    "state_channels = 3  # Number of channels for the input state (RGB)\n",
    "state_height = grid_height\n",
    "state_width = grid_width\n",
    "action_size = 4  # UP, DOWN, LEFT, RIGHT\n",
    "\n",
    "# DQN model (using CNN)\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(state_channels, 16, kernel_size=3, stride=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = DQN()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Snake class\n",
    "class Snake:\n",
    "    def __init__(self):\n",
    "        self.x = random.randint(1, grid_width - 2)\n",
    "        self.y = random.randint(1, grid_height - 2)\n",
    "        self.direction = random.choice(['UP', 'DOWN', 'LEFT', 'RIGHT'])\n",
    "        self.body = [(self.x, self.y)]\n",
    "\n",
    "    def move(self):\n",
    "        if self.direction == \"RIGHT\":\n",
    "            self.x += 1\n",
    "        elif self.direction == \"LEFT\":\n",
    "            self.x -= 1\n",
    "        elif self.direction == \"UP\":\n",
    "            self.y -= 1\n",
    "        elif self.direction == \"DOWN\":\n",
    "            self.y += 1\n",
    "\n",
    "        head = (self.x, self.y)\n",
    "        self.body.insert(0, head)\n",
    "        self.body.pop()\n",
    "\n",
    "    def change_direction(self, new_direction):\n",
    "        if new_direction == 'UP' and self.direction != 'DOWN':\n",
    "            self.direction = new_direction\n",
    "        elif new_direction == 'DOWN' and self.direction != 'UP':\n",
    "            self.direction = new_direction\n",
    "        elif new_direction == 'LEFT' and self.direction != 'RIGHT':\n",
    "            self.direction = new_direction\n",
    "        elif new_direction == 'RIGHT' and self.direction != 'LEFT':\n",
    "            self.direction = new_direction\n",
    "\n",
    "    def draw(self):\n",
    "        for segment in self.body:\n",
    "            pygame.draw.rect(screen, GREEN, (segment[0] * grid_size, segment[1] * grid_size, grid_size, grid_size))\n",
    "\n",
    "# Apple class\n",
    "class Apple:\n",
    "    def __init__(self):\n",
    "        self.x = random.randrange(0, screen_width, grid_size)\n",
    "        self.y = random.randrange(0, screen_height, grid_size)\n",
    "\n",
    "    def draw(self):\n",
    "        pygame.draw.rect(screen, RED, (self.x, self.y, grid_size, grid_size))\n",
    "\n",
    "# Game initialization\n",
    "def initialize_game():\n",
    "    snake = Snake()\n",
    "    apple = Apple()\n",
    "    return snake, apple\n",
    "\n",
    "# Get current state\n",
    "def get_state(snake, apple):\n",
    "    state = np.zeros((grid_width, grid_height))\n",
    "    for segment in snake.body:\n",
    "        x, y = segment[0], segment[1]\n",
    "        if x < grid_width and y < grid_height:\n",
    "            state[y, x] = 1  # Note the change in indexing here\n",
    "    if apple.x < grid_width and apple.y < grid_height:\n",
    "        state[apple.y, apple.x] = 2  # Note the change in indexing here\n",
    "    return preprocess_state(state)\n",
    "\n",
    "# Preprocess state as RGB image\n",
    "def preprocess_state(state):\n",
    "    preprocessed_state = np.zeros((state_channels, state_height, state_width))\n",
    "    preprocessed_state[0, :state.shape[0], :state.shape[1]][state == 1] = 1  # Snake body\n",
    "    preprocessed_state[1, :state.shape[0], :state.shape[1]][state == 2] = 1  # Apple\n",
    "    preprocessed_state[2] = 1  # Background (set all elements to 1)\n",
    "    return preprocessed_state\n",
    "\n",
    "\n",
    "# Select action based on current state\n",
    "def select_action(state):\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float).unsqueeze(0)\n",
    "    q_values = model(state_tensor)\n",
    "    action = torch.argmax(q_values, dim=1).item()\n",
    "    return action\n",
    "\n",
    "# Game over condition\n",
    "def is_game_over(snake):\n",
    "    if snake.x < 0 or snake.x >= grid_width or snake.y < 0 or snake.y >= grid_height:\n",
    "        return True\n",
    "    if (snake.x, snake.y) in snake.body[1:]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Play the game\n",
    "def play_game():\n",
    "    snake, apple = initialize_game()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        state = get_state(snake, apple)\n",
    "        action = select_action(state)\n",
    "        snake.change_direction(['UP', 'DOWN', 'LEFT', 'RIGHT'][action])\n",
    "        snake.move()\n",
    "        if is_game_over(snake):\n",
    "            done = True\n",
    "            reward = -10\n",
    "        elif snake.x == apple.x and snake.y == apple.y:\n",
    "            snake.body.append((apple.x, apple.y))\n",
    "            apple = Apple()\n",
    "            score += 1\n",
    "            reward = 10\n",
    "        else:\n",
    "            reward = -1\n",
    "        next_state = get_state(snake, apple)\n",
    "\n",
    "        # Update Q-values\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float).unsqueeze(0)\n",
    "        next_state_tensor = torch.tensor(next_state, dtype=torch.float).unsqueeze(0)\n",
    "        q_values = model(state_tensor)\n",
    "        next_q_values = model(next_state_tensor)\n",
    "        max_next_q_value = torch.max(next_q_values).item()\n",
    "        q_target = q_values.clone()\n",
    "        q_target[0][action] = reward + 0.99 * max_next_q_value\n",
    "\n",
    "        # Update the model\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(q_values, q_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        screen.fill(BLACK)\n",
    "        snake.draw()\n",
    "        apple.draw()\n",
    "        pygame.display.flip()\n",
    "        clock.tick(10)\n",
    "\n",
    "    return score\n",
    "\n",
    "# Main loop\n",
    "num_episodes = 1000\n",
    "for episode in range(num_episodes):\n",
    "    score = play_game()\n",
    "    print(f\"Episode: {episode + 1}  Score: {score}\")\n",
    "\n",
    "pygame.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.4.0 (SDL 2.26.4, Python 3.8.16)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Episode: 1  Score: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jwshi\\AppData\\Local\\Temp\\ipykernel_6800\\2064483818.py:180: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  states_tensor = torch.tensor(states, dtype=torch.float)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 221\u001b[0m\n\u001b[0;32m    219\u001b[0m num_episodes \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[0;32m    220\u001b[0m \u001b[39mfor\u001b[39;00m episode \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_episodes):\n\u001b[1;32m--> 221\u001b[0m     score \u001b[39m=\u001b[39m play_game()\n\u001b[0;32m    222\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpisode: \u001b[39m\u001b[39m{\u001b[39;00mepisode\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m  Score: \u001b[39m\u001b[39m{\u001b[39;00mscore\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    224\u001b[0m     \u001b[39m# 매 에피소드마다 loss 값을 기록\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 186\u001b[0m, in \u001b[0;36mplay_game\u001b[1;34m()\u001b[0m\n\u001b[0;32m    183\u001b[0m next_states_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(next_states, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\n\u001b[0;32m    184\u001b[0m dones_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(dones, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\n\u001b[1;32m--> 186\u001b[0m states_tensor \u001b[39m=\u001b[39m states_tensor\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m    187\u001b[0m actions_tensor \u001b[39m=\u001b[39m actions_tensor\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m    188\u001b[0m rewards_tensor \u001b[39m=\u001b[39m rewards_tensor\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 게임 초기화\n",
    "pygame.init()\n",
    "screen_width, screen_height = 640, 480\n",
    "screen = pygame.display.set_mode((screen_width, screen_height))\n",
    "pygame.display.set_caption('Snakegame with DQN by SJW')\n",
    "clock = pygame.time.Clock()\n",
    "\n",
    "# 색깔 정의\n",
    "BLACK = (0, 0, 0)\n",
    "GREEN = (0, 255, 0)\n",
    "RED = (255, 0, 0)\n",
    "WHITE = (255, 255, 255)\n",
    "\n",
    "# 게임 환경 변수\n",
    "grid_size = 20\n",
    "grid_width = screen_width // grid_size\n",
    "grid_height = screen_height // grid_size\n",
    "\n",
    "# DQN 관련 변수\n",
    "state_size = (1, grid_width, grid_height)  # 입력 상태의 형태를 (채널 수, 가로 크기, 세로 크기)로 설정\n",
    "action_size = 4  # 상(0), 하(1), 좌(2), 우(3)\n",
    "batch_size = 32\n",
    "memory_size = 1000\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.999\n",
    "epsilon_min = 0.01\n",
    "target_update_freq = 10  # Update target network every n episodes\n",
    "\n",
    "# DQN 신경망 모델 (CNN)\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 256)\n",
    "        self.fc2 = nn.Linear(256, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = DQN()\n",
    "target_model = DQN()\n",
    "target_model.load_state_dict(model.state_dict())  # Initialize target model with main model weights\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Replay Memory\n",
    "memory = []\n",
    "\n",
    "# Snake 클래스 정의\n",
    "class Snake:\n",
    "    def __init__(self):\n",
    "        self.x = random.randint(1, grid_width - 2)\n",
    "        self.y = random.randint(1, grid_height - 2)\n",
    "        self.direction = random.choice(['UP', 'DOWN', 'LEFT', 'RIGHT'])\n",
    "        self.body = [(self.x, self.y)]\n",
    "\n",
    "    def move(self):\n",
    "        if self.direction == \"RIGHT\":\n",
    "            self.x += 1\n",
    "        elif self.direction == \"LEFT\":\n",
    "            self.x -= 1\n",
    "        elif self.direction == \"UP\":\n",
    "            self.y -= 1\n",
    "        elif self.direction == \"DOWN\":\n",
    "            self.y += 1\n",
    "\n",
    "        # 현재 머리의 좌표를 저장\n",
    "        head = (self.x, self.y)\n",
    "\n",
    "        # 머리를 이동시킨 후, 몸통의 좌표 업데이트\n",
    "        self.body.insert(0, head)\n",
    "        self.body.pop()\n",
    "\n",
    "        \n",
    "    def change_direction(self, new_direction):\n",
    "        if new_direction == 'UP' and self.direction != 'DOWN':\n",
    "            self.direction = new_direction\n",
    "        elif new_direction == 'DOWN' and self.direction != 'UP':\n",
    "            self.direction = new_direction\n",
    "        elif new_direction == 'LEFT' and self.direction != 'RIGHT':\n",
    "            self.direction = new_direction\n",
    "        elif new_direction == 'RIGHT' and self.direction != 'LEFT':\n",
    "            self.direction = new_direction\n",
    "\n",
    "    def draw(self):\n",
    "        for segment in self.body:\n",
    "            pygame.draw.rect(screen, GREEN, (segment[0] * grid_size, segment[1] * grid_size, grid_size, grid_size))\n",
    "\n",
    "# Apple 클래스 정의\n",
    "class Apple:\n",
    "    def __init__(self):\n",
    "        self.x = random.randrange(0, screen_width, 10) \n",
    "        self.y = random.randrange(0, screen_height, 10) \n",
    "\n",
    "    def draw(self):\n",
    "        pygame.draw.rect(screen, RED, (self.x * grid_size, self.y * grid_size, grid_size, grid_size))\n",
    "\n",
    "# 게임 초기화\n",
    "def initialize_game():\n",
    "    snake = Snake()\n",
    "    apple = Apple()\n",
    "    return snake, apple\n",
    "\n",
    "# 상태를 픽셀 그리드로 변환\n",
    "def get_state(snake, apple):\n",
    "    state = np.zeros((1, grid_width, grid_height))\n",
    "    for segment in snake.body:\n",
    "        x, y = segment[0], segment[1]\n",
    "        if x < grid_width and y < grid_height:\n",
    "            state[0, x, y] = 1\n",
    "    if apple.x < grid_width and apple.y < grid_height:\n",
    "        state[0, apple.x, apple.y] = 2\n",
    "    return state\n",
    "\n",
    "\n",
    "# DQN 모델로 행동 선택\n",
    "def select_action(state):\n",
    "    if random.random() < epsilon:\n",
    "        action = random.randint(0, action_size - 1)\n",
    "    else:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float).unsqueeze(0)\n",
    "        q_values = model(state_tensor)\n",
    "        action = torch.argmax(q_values, dim=1).item()\n",
    "    return action\n",
    "\n",
    "# 게임 종료 여부 확인\n",
    "def is_game_over(snake):\n",
    "    if snake.x < 0 or snake.x >= grid_width or snake.y < 0 or snake.y >= grid_height:\n",
    "        return True\n",
    "    if (snake.x, snake.y) in snake.body[1:]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# 게임 진행\n",
    "def play_game():\n",
    "    global epsilon, loss\n",
    "    snake, apple = initialize_game()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        state = get_state(snake, apple)\n",
    "        action = select_action(state)\n",
    "        snake.change_direction(['UP', 'DOWN', 'LEFT', 'RIGHT'][action])\n",
    "        snake.move()\n",
    "        if is_game_over(snake):\n",
    "            done = True\n",
    "            reward = -10\n",
    "        elif snake.x == apple.x and snake.y == apple.y:\n",
    "            snake.body.append((apple.x, apple.y))\n",
    "            apple = Apple()\n",
    "            score += 1\n",
    "            reward = 10\n",
    "        else:\n",
    "            reward = -1\n",
    "        next_state = get_state(snake, apple)\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "        # Replay Memory 크기 제한\n",
    "        if len(memory) > memory_size:\n",
    "            memory.pop(0)\n",
    "\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = random.sample(memory, batch_size)\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "            states_tensor = torch.tensor(states, dtype=torch.float)\n",
    "            actions_tensor = torch.tensor(actions, dtype=torch.long).unsqueeze(1)\n",
    "            rewards_tensor = torch.tensor(rewards, dtype=torch.float)\n",
    "            next_states_tensor = torch.tensor(next_states, dtype=torch.float)\n",
    "            dones_tensor = torch.tensor(dones, dtype=torch.float)\n",
    "\n",
    "            states_tensor = states_tensor.to(device)\n",
    "            actions_tensor = actions_tensor.to(device)\n",
    "            rewards_tensor = rewards_tensor.to(device)\n",
    "            next_states_tensor = next_states_tensor.to(device)\n",
    "            dones_tensor = dones_tensor.to(device)\n",
    "\n",
    "            q_values = model(states_tensor)\n",
    "            next_q_values = target_model(next_states_tensor)\n",
    "\n",
    "            q_targets = rewards_tensor + 0.99 * torch.max(next_q_values, dim=1)[0] * (1 - dones_tensor)\n",
    "            q_targets = q_targets.unsqueeze(1)\n",
    "\n",
    "            q_values = torch.gather(q_values, 1, actions_tensor)\n",
    "            loss = loss_fn(q_values, q_targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        screen.fill(BLACK)\n",
    "        snake.draw()\n",
    "        apple.draw()\n",
    "        pygame.display.flip()\n",
    "        clock.tick(10)\n",
    "\n",
    "    return score\n",
    "\n",
    "# 학습 진행 상황을 기록하기 위한 리스트\n",
    "losses = []\n",
    "# 전역 변수로 loss 선언\n",
    "loss = None\n",
    "\n",
    "# 학습\n",
    "num_episodes = 1000\n",
    "for episode in range(num_episodes):\n",
    "    score = play_game()\n",
    "    print(f\"Episode: {episode + 1}  Score: {score}\")\n",
    "\n",
    "    # 매 에피소드마다 loss 값을 기록\n",
    "    if loss is not None:\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # 타겟 네트워크 업데이트\n",
    "    if episode % target_update_freq == 0:\n",
    "        target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "    # 탐험 비율 감소\n",
    "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "\n",
    "    # 에피소드가 100의 배수일 때마다 그래프 그리기\n",
    "    if episode % 100 == 0 and episode > 0:\n",
    "        plt.plot(losses)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('DQN Training')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pygame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jwshi\\Desktop\\Data\\py38\\ysda\\study_with_code\\Assignments\\중프과제.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jwshi/Desktop/Data/py38/ysda/study_with_code/Assignments/%EC%A4%91%ED%94%84%EA%B3%BC%EC%A0%9C.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpygame\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jwshi/Desktop/Data/py38/ysda/study_with_code/Assignments/%EC%A4%91%ED%94%84%EA%B3%BC%EC%A0%9C.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jwshi/Desktop/Data/py38/ysda/study_with_code/Assignments/%EC%A4%91%ED%94%84%EA%B3%BC%EC%A0%9C.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pygame'"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 게임 초기화\n",
    "pygame.init()\n",
    "screen_width, screen_height = 640, 480\n",
    "screen = pygame.display.set_mode((screen_width, screen_height))\n",
    "pygame.display.set_caption('Snakegame with DQN by SJW')\n",
    "clock = pygame.time.Clock()\n",
    "\n",
    "# 색깔 정의\n",
    "BLACK = (0, 0, 0)\n",
    "GREEN = (0, 255, 0)\n",
    "RED = (255, 0, 0)\n",
    "WHITE = (255, 255, 255)\n",
    "\n",
    "# 게임 환경 변수\n",
    "grid_size = 20\n",
    "grid_width = screen_width // grid_size\n",
    "grid_height = screen_height // grid_size\n",
    "\n",
    "# DQN 관련 변수\n",
    "state_size = grid_width * grid_height\n",
    "action_size = 4  # 상(0), 하(1), 좌(2), 우(3)\n",
    "batch_size = 32\n",
    "memory_size = 1000\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.999\n",
    "epsilon_min = 0.01\n",
    "target_update_freq = 10  # Update target network every n episodes\n",
    "\n",
    "# DQN 신경망 모델\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 24)\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        self.fc3 = nn.Linear(24, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = DQN()\n",
    "target_model = DQN()\n",
    "target_model.load_state_dict(model.state_dict())  # Initialize target model with main model weights\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Replay Memory\n",
    "memory = []\n",
    "\n",
    "# Snake 클래스 정의\n",
    "class Snake:\n",
    "    def __init__(self):\n",
    "        self.x = random.randint(1, grid_width - 2)\n",
    "        self.y = random.randint(1, grid_height - 2)\n",
    "        self.direction = random.choice(['UP', 'DOWN', 'LEFT', 'RIGHT'])\n",
    "        self.body = [(self.x, self.y)]\n",
    "\n",
    "    def move(self):\n",
    "        if self.direction == \"RIGHT\":\n",
    "            self.x += 1\n",
    "        elif self.direction == \"LEFT\":\n",
    "            self.x -= 1\n",
    "        elif self.direction == \"UP\":\n",
    "            self.y -= 1\n",
    "        elif self.direction == \"DOWN\":\n",
    "            self.y += 1\n",
    "\n",
    "        # 현재 머리의 좌표를 저장\n",
    "        head = (self.x, self.y)\n",
    "\n",
    "        # 머리를 이동시킨 후, 몸통의 좌표 업데이트\n",
    "        self.body.insert(0, head)\n",
    "        self.body.pop()\n",
    "\n",
    "        \n",
    "    def change_direction(self, new_direction):\n",
    "        if new_direction == 'UP' and self.direction != 'DOWN':\n",
    "            self.direction = new_direction\n",
    "        elif new_direction == 'DOWN' and self.direction != 'UP':\n",
    "            self.direction = new_direction\n",
    "        elif new_direction == 'LEFT' and self.direction != 'RIGHT':\n",
    "            self.direction = new_direction\n",
    "        elif new_direction == 'RIGHT' and self.direction != 'LEFT':\n",
    "            self.direction = new_direction\n",
    "\n",
    "    def draw(self):\n",
    "        for segment in self.body:\n",
    "            pygame.draw.rect(screen, GREEN, (segment[0] * grid_size, segment[1] * grid_size, grid_size, grid_size))\n",
    "\n",
    "# Apple 클래스 정의\n",
    "class Apple:\n",
    "    def __init__(self):\n",
    "        self.x = random.randrange(0, screen_width, 10) \n",
    "        self.y = random.randrange(0, screen_height, 10) \n",
    "\n",
    "    def draw(self):\n",
    "        pygame.draw.rect(screen, RED, (self.x * grid_size, self.y * grid_size, grid_size, grid_size))\n",
    "\n",
    "# 게임 초기화\n",
    "def initialize_game():\n",
    "    snake = Snake()\n",
    "    apple = Apple()\n",
    "    return snake, apple\n",
    "\n",
    "# 상태를 픽셀 그리드로 변환\n",
    "def get_state(snake, apple):\n",
    "    state = np.zeros((grid_width, grid_height))\n",
    "    for segment in snake.body:\n",
    "        x, y = segment[0], segment[1]\n",
    "        if x < grid_width and y < grid_height:\n",
    "            state[x, y] = 1\n",
    "    if apple.x < grid_width and apple.y < grid_height:\n",
    "        state[apple.x, apple.y] = 2\n",
    "    return state.flatten()\n",
    "\n",
    "\n",
    "# DQN 모델로 행동 선택\n",
    "def select_action(state):\n",
    "    if random.random() < epsilon:\n",
    "        action = random.randint(0, action_size - 1)\n",
    "    else:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float).unsqueeze(0)\n",
    "        q_values = model(state_tensor)\n",
    "        action = torch.argmax(q_values, dim=1).item()\n",
    "    return action\n",
    "\n",
    "# 게임 종료 여부 확인\n",
    "def is_game_over(snake):\n",
    "    if snake.x < 0 or snake.x >= grid_width or snake.y < 0 or snake.y >= grid_height:\n",
    "        return True\n",
    "    if (snake.x, snake.y) in snake.body[1:]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# 게임 진행\n",
    "def play_game():\n",
    "    global epsilon, loss\n",
    "    snake, apple = initialize_game()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        state = get_state(snake, apple)\n",
    "        action = select_action(state)\n",
    "        snake.change_direction(['UP', 'DOWN', 'LEFT', 'RIGHT'][action])\n",
    "        snake.move()\n",
    "        if is_game_over(snake):\n",
    "            done = True\n",
    "            reward = -10\n",
    "        elif snake.x == apple.x and snake.y == apple.y:\n",
    "            snake.body.append((apple.x, apple.y))\n",
    "            apple = Apple()\n",
    "            score += 1\n",
    "            reward = 10\n",
    "        else:\n",
    "            reward = -1\n",
    "        next_state = get_state(snake, apple)\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "        # Replay Memory 크기 제한\n",
    "        if len(memory) > memory_size:\n",
    "            memory.pop(0)\n",
    "\n",
    "        if len(memory) >= batch_size:\n",
    "            batch = random.sample(memory, batch_size)\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "            states_tensor = torch.tensor(states, dtype=torch.float)\n",
    "            actions_tensor = torch.tensor(actions, dtype=torch.long).unsqueeze(1)\n",
    "            rewards_tensor = torch.tensor(rewards, dtype=torch.float)\n",
    "            next_states_tensor = torch.tensor(next_states, dtype=torch.float)\n",
    "            dones_tensor = torch.tensor(dones, dtype=torch.float)\n",
    "\n",
    "            states_tensor = states_tensor.to(device)\n",
    "            actions_tensor = actions_tensor.to(device)\n",
    "            rewards_tensor = rewards_tensor.to(device)\n",
    "            next_states_tensor = next_states_tensor.to(device)\n",
    "            dones_tensor = dones_tensor.to(device)\n",
    "\n",
    "            q_values = model(states_tensor)\n",
    "            next_q_values = target_model(next_states_tensor)\n",
    "\n",
    "            q_targets = rewards_tensor + 0.99 * torch.max(next_q_values, dim=1)[0] * (1 - dones_tensor)\n",
    "            q_targets = q_targets.unsqueeze(1)\n",
    "\n",
    "            q_values = torch.gather(q_values, 1, actions_tensor)\n",
    "            loss = loss_fn(q_values, q_targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        screen.fill(BLACK)\n",
    "        snake.draw()\n",
    "        apple.draw()\n",
    "        pygame.display.flip()\n",
    "        clock.tick(10)\n",
    "\n",
    "    return score\n",
    "\n",
    "# 학습 진행 상황을 기록하기 위한 리스트\n",
    "losses = []\n",
    "# 전역 변수로 loss 선언\n",
    "loss = None\n",
    "\n",
    "# 학습\n",
    "num_episodes = 1000\n",
    "for episode in range(num_episodes):\n",
    "    score = play_game()\n",
    "    print(f\"Episode: {episode + 1}  Score: {score}\")\n",
    "\n",
    "    # 매 에피소드마다 loss 값을 기록\n",
    "    if loss is not None:\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # 타겟 네트워크 업데이트\n",
    "    if episode % target_update_freq == 0:\n",
    "        target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "    # 탐험 비율 감소\n",
    "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "\n",
    "    # 그래프 그리기\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('DQN Training')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'epsilon' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 237\u001b[0m\n\u001b[0;32m    234\u001b[0m         pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mupdate()\n\u001b[0;32m    235\u001b[0m         clock\u001b[39m.\u001b[39mtick(\u001b[39m10\u001b[39m)\n\u001b[1;32m--> 237\u001b[0m game_loop()\n",
      "Cell \u001b[1;32mIn[6], line 176\u001b[0m, in \u001b[0;36mgame_loop\u001b[1;34m()\u001b[0m\n\u001b[0;32m    173\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m    175\u001b[0m episode_rewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[1;32m--> 176\u001b[0m epsilon_values\u001b[39m.\u001b[39mappend(epsilon)\n\u001b[0;32m    178\u001b[0m memory\u001b[39m.\u001b[39mappend((state, snake\u001b[39m.\u001b[39mdirection, reward, next_state, done))\n\u001b[0;32m    179\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(memory) \u001b[39m>\u001b[39m memory_size:\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'epsilon' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Game initialization\n",
    "pygame.init()\n",
    "screen_width, screen_height = 640, 480\n",
    "screen = pygame.display.set_mode((screen_width, screen_height))\n",
    "pygame.display.set_caption('Snakegame with DQN by SJW')\n",
    "clock = pygame.time.Clock()\n",
    "\n",
    "# Colors\n",
    "BLACK = (0, 0, 0)\n",
    "GREEN = (0, 255, 0)\n",
    "RED = (255, 0, 0)\n",
    "WHITE = (255, 255, 255)\n",
    "\n",
    "# Game environment variables\n",
    "grid_size = 20\n",
    "grid_width = screen_width // grid_size\n",
    "grid_height = screen_height // grid_size\n",
    "\n",
    "# DQN variables\n",
    "state_size = (1, grid_width, grid_height)  # Set the shape of the input state as (channel, width, height)\n",
    "action_size = 4  # Up(0), Down(1), Left(2), Right(3)\n",
    "batch_size = 32\n",
    "memory_size = 1000\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.999\n",
    "epsilon_min = 0.01\n",
    "target_update_freq = 10  # Update target network every n episodes\n",
    "\n",
    "# DQN model (CNN)\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 256)\n",
    "        self.fc2 = nn.Linear(256, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# GPU acceleration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DQN().to(device)\n",
    "target_model = DQN().to(device)\n",
    "target_model.load_state_dict(model.state_dict())\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Replay memory\n",
    "memory = []\n",
    "\n",
    "# Snake class\n",
    "class Snake:\n",
    "    def __init__(self):\n",
    "        self.x = random.randint(1, grid_width - 2)\n",
    "        self.y = random.randint(1, grid_height - 2)\n",
    "        self.direction = random.choice(['UP', 'DOWN', 'LEFT', 'RIGHT'])\n",
    "        self.body = [(self.x, self.y)]\n",
    "\n",
    "    def move(self):\n",
    "        if self.direction == \"RIGHT\":\n",
    "            self.x += 1\n",
    "        elif self.direction == \"LEFT\":\n",
    "            self.x -= 1\n",
    "        elif self.direction == \"UP\":\n",
    "            self.y -= 1\n",
    "        elif self.direction == \"DOWN\":\n",
    "            self.y += 1\n",
    "\n",
    "        head = (self.x, self.y)\n",
    "        self.body.insert(0, head)\n",
    "        self.body.pop()\n",
    "\n",
    "    def change_direction(self, new_direction):\n",
    "        if new_direction == 'UP' and self.direction != 'DOWN':\n",
    "            self.direction = new_direction\n",
    "        elif new_direction == 'DOWN' and self.direction != 'UP':\n",
    "            self.direction = new_direction\n",
    "        elif new_direction == 'LEFT' and self.direction != 'RIGHT':\n",
    "            self.direction = new_direction\n",
    "        elif new_direction == 'RIGHT' and self.direction != 'LEFT':\n",
    "            self.direction = new_direction\n",
    "\n",
    "    def draw(self):\n",
    "        snake_segments = [(segment[0] * grid_size, segment[1] * grid_size, grid_size, grid_size) for segment in self.body]\n",
    "        pygame.draw.rects(screen, GREEN, snake_segments)\n",
    "\n",
    "    def check_collision(self):\n",
    "        if self.x < 0 or self.x >= grid_width or self.y < 0 or self.y >= grid_height:\n",
    "            return True\n",
    "        if (self.x, self.y) in self.body[1:]:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def check_eat(self, apple):\n",
    "        if self.x == apple.x and self.y == apple.y:\n",
    "            self.body.append((self.x, self.y))\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "# Apple class\n",
    "class Apple:\n",
    "    def __init__(self):\n",
    "        self.x = random.randint(0, grid_width - 1)\n",
    "        self.y = random.randint(0, grid_height - 1)\n",
    "\n",
    "    def draw(self):\n",
    "        pygame.draw.rect(screen, RED, (self.x * grid_size, self.y * grid_size, grid_size, grid_size))\n",
    "\n",
    "# Game loop\n",
    "def game_loop():\n",
    "    snake = Snake()\n",
    "    apple = Apple()\n",
    "\n",
    "    state_grid = np.zeros((grid_width, grid_height), dtype=np.int64)\n",
    "    state_grid.fill(0)\n",
    "    state = np.expand_dims(state_grid, axis=0)\n",
    "\n",
    "    episode_rewards = []\n",
    "    epsilon_values = []\n",
    "\n",
    "    episode_count = 0\n",
    "\n",
    "    while True:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                quit()\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_UP:\n",
    "                    snake.change_direction('UP')\n",
    "                elif event.key == pygame.K_DOWN:\n",
    "                    snake.change_direction('DOWN')\n",
    "                elif event.key == pygame.K_LEFT:\n",
    "                    snake.change_direction('LEFT')\n",
    "                elif event.key == pygame.K_RIGHT:\n",
    "                    snake.change_direction('RIGHT')\n",
    "\n",
    "        snake.move()\n",
    "\n",
    "        state_grid.fill(0)\n",
    "        for segment in snake.body:\n",
    "            x, y = segment[0], segment[1]\n",
    "            if x < grid_width and y < grid_height:\n",
    "                state_grid[x, y] = 1\n",
    "        if apple.x < grid_width and apple.y < grid_height:\n",
    "            state_grid[apple.x, apple.y] = 2\n",
    "        next_state = np.expand_dims(state_grid, axis=0)\n",
    "\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        if snake.check_collision():\n",
    "            reward = -10\n",
    "            done = True\n",
    "        elif snake.check_eat(apple):\n",
    "            reward = 10\n",
    "            apple = Apple()\n",
    "        else:\n",
    "            reward = -1\n",
    "\n",
    "        episode_rewards.append(reward)\n",
    "        epsilon_values.append(epsilon)\n",
    "\n",
    "        memory.append((state, snake.direction, reward, next_state, done))\n",
    "        if len(memory) > memory_size:\n",
    "            memory.pop(0)\n",
    "\n",
    "        if len(memory) >= batch_size:\n",
    "            batch_indices = np.random.choice(len(memory), size=batch_size, replace=False)\n",
    "            batch = [memory[i] for i in batch_indices]\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "            states_tensor = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "            actions_tensor = torch.tensor(actions, dtype=torch.long, device=device)\n",
    "            rewards_tensor = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "            next_states_tensor = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
    "            dones_tensor = torch.tensor(dones, dtype=torch.float32, device=device)\n",
    "\n",
    "            q_values = model(states_tensor)\n",
    "            q_targets = q_values.clone().detach()\n",
    "\n",
    "            next_q_values = target_model(next_states_tensor)\n",
    "            next_q_values_max = torch.max(next_q_values, dim=1)[0]\n",
    "            q_targets[range(batch_size), actions_tensor] = rewards_tensor + (1 - dones_tensor) * next_q_values_max\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(q_values, q_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            episode_count += 1\n",
    "\n",
    "            if episode_count % target_update_freq == 0:\n",
    "                target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "            if epsilon > epsilon_min:\n",
    "                epsilon *= epsilon_decay\n",
    "\n",
    "            if episode_count % 100 == 0:\n",
    "                plt.plot(episode_rewards)\n",
    "                plt.xlabel('Episode')\n",
    "                plt.ylabel('Reward')\n",
    "                plt.title('Episode Rewards')\n",
    "                plt.show()\n",
    "\n",
    "            snake = Snake()\n",
    "            apple = Apple()\n",
    "\n",
    "            state_grid.fill(0)\n",
    "            state = np.expand_dims(state_grid, axis=0)\n",
    "\n",
    "            episode_rewards = []\n",
    "\n",
    "        screen.fill(BLACK)\n",
    "        snake.draw()\n",
    "        apple.draw()\n",
    "        pygame.display.update()\n",
    "        clock.tick(10)\n",
    "\n",
    "game_loop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 215\u001b[0m\n\u001b[0;32m    213\u001b[0m actions_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(batch_size, action_size)\n\u001b[0;32m    214\u001b[0m \u001b[39mfor\u001b[39;00m i, action \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(actions):\n\u001b[1;32m--> 215\u001b[0m     actions_tensor[i][action] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    217\u001b[0m rewards_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(rewards, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m    218\u001b[0m next_states_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(next_states, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Game initialization\n",
    "pygame.init()\n",
    "screen_width, screen_height = 640, 480\n",
    "screen = pygame.display.set_mode((screen_width, screen_height))\n",
    "pygame.display.set_caption('Snakegame with DQN by SJW')\n",
    "clock = pygame.time.Clock()\n",
    "\n",
    "# Color definition\n",
    "BLACK = (0, 0, 0)\n",
    "GREEN = (0, 255, 0)\n",
    "RED = (255, 0, 0)\n",
    "WHITE = (255, 255, 255)\n",
    "\n",
    "# Game environment variables\n",
    "grid_size = 20\n",
    "grid_width = screen_width // grid_size\n",
    "grid_height = screen_height // grid_size\n",
    "\n",
    "# DQN related variables\n",
    "state_size = (1, grid_width, grid_height)  # Set the shape of the input state as (channel, width, height)\n",
    "action_size = 4  # Up(0), Down(1), Left(2), Right(3)\n",
    "batch_size = 32\n",
    "memory_size = 1000\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.999\n",
    "epsilon_min = 0.01\n",
    "target_update_freq = 10  # Update target network every n episodes\n",
    "\n",
    "# DQN neural network model (CNN)\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1)\n",
    "        \n",
    "        # Calculate the size of the flattened tensor after the convolutional layers\n",
    "        conv_out_size = self._get_conv_out_size(state_size)\n",
    "        \n",
    "        self.fc1 = nn.Linear(conv_out_size, 256)\n",
    "        self.fc2 = nn.Linear(256, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def _get_conv_out_size(self, shape):\n",
    "        o = self.conv1(torch.zeros(1, *shape))\n",
    "        o = self.conv2(o)\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "model = DQN()\n",
    "target_model = DQN()\n",
    "target_model.load_state_dict(model.state_dict())  # Initialize target model with main model weights\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Replay Memory\n",
    "memory = []\n",
    "\n",
    "# Snake class definition\n",
    "class Snake:\n",
    "    def __init__(self):\n",
    "        self.x = random.randint(1, grid_width - 2)\n",
    "        self.y = random.randint(1, grid_height - 2)\n",
    "        self.direction = random.choice(['UP', 'DOWN', 'LEFT', 'RIGHT'])\n",
    "        self.body = [(self.x, self.y)]\n",
    "\n",
    "    def move(self):\n",
    "        if self.direction == \"RIGHT\":\n",
    "            self.x += 1\n",
    "        elif self.direction == \"LEFT\":\n",
    "            self.x -= 1\n",
    "        elif self.direction == \"UP\":\n",
    "            self.y -= 1\n",
    "        elif self.direction == \"DOWN\":\n",
    "            self.y += 1\n",
    "\n",
    "        # Save the current head position\n",
    "        head = (self.x, self.y)\n",
    "\n",
    "        # Update the body positions after moving the head\n",
    "        self.body.insert(0, head)\n",
    "        self.body.pop()\n",
    "\n",
    "    def change_direction(self, new_direction):\n",
    "        if new_direction == 'UP' and self.direction != 'DOWN':\n",
    "            self.direction = new_direction\n",
    "        elif new_direction == 'DOWN' and self.direction != 'UP':\n",
    "            self.direction = new_direction\n",
    "        elif new_direction == 'LEFT' and self.direction != 'RIGHT':\n",
    "            self.direction = new_direction\n",
    "        elif new_direction == 'RIGHT' and self.direction != 'LEFT':\n",
    "            self.direction = new_direction\n",
    "\n",
    "    def draw(self):\n",
    "        for segment in self.body:\n",
    "            pygame.draw.rect(screen, GREEN, (segment[0] * grid_size, segment[1] * grid_size, grid_size, grid_size))\n",
    "\n",
    "# Apple class definition\n",
    "class Apple:\n",
    "    def __init__(self):\n",
    "        self.x = random.randrange(0, screen_width, 10)\n",
    "        self.y = random.randrange(0, screen_height, 10)\n",
    "\n",
    "    def draw(self):\n",
    "        pygame.draw.rect(screen, RED, (self.x * grid_size, self.y * grid_size, grid_size, grid_size))\n",
    "\n",
    "# Game initialization\n",
    "def initialize_game():\n",
    "    snake = Snake()\n",
    "    apple = Apple()\n",
    "    return snake, apple\n",
    "\n",
    "# Convert the state to pixel grid\n",
    "def get_state(snake, apple):\n",
    "    state = np.zeros((1, grid_width, grid_height))\n",
    "    for segment in snake.body:\n",
    "        x, y = segment[0], segment[1]\n",
    "        if x < grid_width and y < grid_height:\n",
    "            state[0, x, y] = 1\n",
    "    if apple.x < grid_width and apple.y < grid_height:\n",
    "        state[0, apple.x, apple.y] = 2\n",
    "    return state\n",
    "\n",
    "# Select action using the DQN model\n",
    "def select_action(state):\n",
    "    if random.random() < epsilon:\n",
    "        action = random.randint(0, action_size - 1)\n",
    "    else:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float).unsqueeze(0)\n",
    "        q_values = model(state_tensor)\n",
    "        action = torch.argmax(q_values, dim=1).item()\n",
    "    return action\n",
    "\n",
    "# Check if the game is over\n",
    "def is_game_over(snake):\n",
    "    if snake.x < 0 or snake.x >= grid_width or snake.y < 0 or snake.y >= grid_height:\n",
    "        return True\n",
    "    if (snake.x, snake.y) in snake.body[1:]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Game loop\n",
    "def game_loop():\n",
    "    global epsilon\n",
    "    snake = Snake()\n",
    "    apple = Apple()  # Create an initial apple\n",
    "    done = False\n",
    "    episode_rewards = []\n",
    "    epsilon_values = []\n",
    "    score = 0  # Initialize the score\n",
    "    \n",
    "    while not done:\n",
    "        state = get_state(snake, apple)  # Get the state\n",
    "        action = select_action(state)\n",
    "        snake.change_direction(['UP', 'DOWN', 'LEFT', 'RIGHT'][action])\n",
    "        snake.move()\n",
    "        \n",
    "        if is_game_over(snake):\n",
    "            done = True\n",
    "            reward = -10\n",
    "        elif snake.x == apple.x and snake.y == apple.y:\n",
    "            snake.body.append((apple.x, apple.y))\n",
    "            score += 1  # Increment the score\n",
    "            apple = Apple()  # Regenerate the apple\n",
    "            reward = 10\n",
    "        else:\n",
    "            reward = -1\n",
    "        \n",
    "        next_state = get_state(snake, apple)  # Get the next state\n",
    "        memory.append((state, snake.direction, reward, next_state, done))\n",
    "        \n",
    "        if len(memory) > memory_size:\n",
    "            memory.pop(0)\n",
    "        \n",
    "        episode_rewards.append(reward)\n",
    "        epsilon_values.append(epsilon)\n",
    "\n",
    "        screen.fill(BLACK)\n",
    "        snake.draw()\n",
    "        apple.draw()\n",
    "        pygame.display.update()\n",
    "        clock.tick(10)\n",
    "\n",
    "    return episode_rewards, epsilon_values, score\n",
    "\n",
    "# Training\n",
    "num_episodes = 1000\n",
    "losses = []\n",
    "plot_interval = 100\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    episode_rewards, epsilon_values,score = game_loop()\n",
    "    if len(memory) >= batch_size:\n",
    "        batch = random.sample(memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states_tensor = torch.tensor(states, dtype=torch.float)\n",
    "        \n",
    "        # Convert actions to one-hot encoded tensors\n",
    "        actions_tensor = torch.zeros(batch_size, action_size)\n",
    "        for i, action in enumerate(actions):\n",
    "            actions_tensor[i][action] = 1\n",
    "\n",
    "        rewards_tensor = torch.tensor(rewards, dtype=torch.float).unsqueeze(1)\n",
    "        next_states_tensor = torch.tensor(next_states, dtype=torch.float)\n",
    "        dones_tensor = torch.tensor(dones, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        q_values = model(states_tensor)\n",
    "        next_q_values = target_model(next_states_tensor)\n",
    "        q_value = q_values.gather(1, actions_tensor.long())\n",
    "        next_q_value = next_q_values.max(1)[0].unsqueeze(1)\n",
    "        expected_q_value = rewards_tensor + next_q_value * (1 - dones_tensor)\n",
    "\n",
    "        loss = loss_fn(q_value, expected_q_value)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    if episode % target_update_freq == 0:\n",
    "        target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "\n",
    "    if episode % plot_interval == 0:\n",
    "        plt.plot(losses)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('DQN Training Loss')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Plotting the loss\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('DQN Training Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pong game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't convert non-rectangular Python sequence to Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 54\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mfor\u001b[39;00m episode \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_episodes):\n\u001b[0;32m     53\u001b[0m     state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m---> 54\u001b[0m     state \u001b[39m=\u001b[39m preprocess_frame(state)\n\u001b[0;32m     55\u001b[0m     state \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(state, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     57\u001b[0m     done \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 28\u001b[0m, in \u001b[0;36mpreprocess_frame\u001b[1;34m(frame)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_frame\u001b[39m(frame):\n\u001b[0;32m     27\u001b[0m     \u001b[39m# Convert to grayscale\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m     frame \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mimage\u001b[39m.\u001b[39;49mrgb_to_grayscale(frame)\n\u001b[0;32m     29\u001b[0m     \u001b[39m# Resize to 84x84\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     frame \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mresize(frame, (\u001b[39m84\u001b[39m, \u001b[39m84\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\jwshi\\miniconda3\\envs\\pygame\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\jwshi\\miniconda3\\envs\\pygame\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    101\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    102\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Can't convert non-rectangular Python sequence to Tensor."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create an Atari game environment\n",
    "env = gym.make(\"ALE/Breakout-v5\")\n",
    "\n",
    "# Define the deep Q-network\n",
    "input_shape = env.observation_space.shape\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=8, strides=4, activation='relu', input_shape=input_shape),\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=4, strides=2, activation='relu'),\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=3, strides=1, activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_actions)\n",
    "])\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.00025, rho=0.95, epsilon=0.01)\n",
    "loss_fn = tf.keras.losses.Huber()\n",
    "\n",
    "# Function to preprocess game frames\n",
    "def preprocess_frame(frame):\n",
    "    # Convert to grayscale\n",
    "    frame = tf.image.rgb_to_grayscale(frame)\n",
    "    # Resize to 84x84\n",
    "    frame = tf.image.resize(frame, (84, 84))\n",
    "    # Normalize pixel values\n",
    "    frame = frame / 255.0\n",
    "    return frame\n",
    "\n",
    "# Function to select an action based on epsilon-greedy policy\n",
    "def select_action(state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.action_space.sample()  # Random action\n",
    "    else:\n",
    "        q_values = model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "# Hyperparameters\n",
    "num_episodes = 10000\n",
    "batch_size = 32\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.999\n",
    "\n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = preprocess_frame(state)\n",
    "    state = np.expand_dims(state, axis=0)\n",
    "\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Select an action\n",
    "        action = select_action(state, epsilon)\n",
    "\n",
    "        # Take the action and observe the next state and reward\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = preprocess_frame(next_state)\n",
    "        next_state = np.expand_dims(next_state, axis=0)\n",
    "\n",
    "        # Store the transition in the replay buffer\n",
    "\n",
    "        # Update the current state\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    # Print the total reward achieved in the episode\n",
    "    print(\"Episode:\", episode, \"Total Reward:\", total_reward)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('dqn_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "box2D is not installed, run `pip install gym[box2d]`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jwshi\\miniconda3\\envs\\pygame\\lib\\site-packages\\gym\\envs\\box2d\\bipedal_walker.py:14\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 14\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mBox2D\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mBox2D\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mb2\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m         circleShape,\n\u001b[0;32m     17\u001b[0m         contactListener,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m         revoluteJointDef,\n\u001b[0;32m     22\u001b[0m     )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Box2D'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgym\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39;49mmake(\u001b[39m\"\u001b[39;49m\u001b[39mLunarLander-v2\u001b[39;49m\u001b[39m\"\u001b[39;49m, render_mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mhuman\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m observation, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset(seed\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\jwshi\\miniconda3\\envs\\pygame\\lib\\site-packages\\gym\\envs\\registration.py:581\u001b[0m, in \u001b[0;36mmake\u001b[1;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[0;32m    578\u001b[0m     env_creator \u001b[39m=\u001b[39m spec_\u001b[39m.\u001b[39mentry_point\n\u001b[0;32m    579\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    580\u001b[0m     \u001b[39m# Assume it's a string\u001b[39;00m\n\u001b[1;32m--> 581\u001b[0m     env_creator \u001b[39m=\u001b[39m load(spec_\u001b[39m.\u001b[39;49mentry_point)\n\u001b[0;32m    583\u001b[0m mode \u001b[39m=\u001b[39m _kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrender_mode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    584\u001b[0m apply_human_rendering \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jwshi\\miniconda3\\envs\\pygame\\lib\\site-packages\\gym\\envs\\registration.py:61\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Loads an environment with name and returns an environment creation function\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \n\u001b[0;32m     54\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39m    Calls the environment constructor\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     60\u001b[0m mod_name, attr_name \u001b[39m=\u001b[39m name\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 61\u001b[0m mod \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(mod_name)\n\u001b[0;32m     62\u001b[0m fn \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(mod, attr_name)\n\u001b[0;32m     63\u001b[0m \u001b[39mreturn\u001b[39;00m fn\n",
      "File \u001b[1;32mc:\\Users\\jwshi\\miniconda3\\envs\\pygame\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:961\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:219\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:843\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:219\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\jwshi\\miniconda3\\envs\\pygame\\lib\\site-packages\\gym\\envs\\box2d\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbox2d\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbipedal_walker\u001b[39;00m \u001b[39mimport\u001b[39;00m BipedalWalker, BipedalWalkerHardcore\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbox2d\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcar_racing\u001b[39;00m \u001b[39mimport\u001b[39;00m CarRacing\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbox2d\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlunar_lander\u001b[39;00m \u001b[39mimport\u001b[39;00m LunarLander, LunarLanderContinuous\n",
      "File \u001b[1;32mc:\\Users\\jwshi\\miniconda3\\envs\\pygame\\lib\\site-packages\\gym\\envs\\box2d\\bipedal_walker.py:24\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mBox2D\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mb2\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m         circleShape,\n\u001b[0;32m     17\u001b[0m         contactListener,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m         revoluteJointDef,\n\u001b[0;32m     22\u001b[0m     )\n\u001b[0;32m     23\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m---> 24\u001b[0m     \u001b[39mraise\u001b[39;00m DependencyNotInstalled(\u001b[39m\"\u001b[39m\u001b[39mbox2D is not installed, run `pip install gym[box2d]`\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[39mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     28\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mpygame\u001b[39;00m\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m: box2D is not installed, run `pip install gym[box2d]`"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "   action = policy(observation)  # User-defined policy function\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
